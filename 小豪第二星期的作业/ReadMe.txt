程序说明


师兄，这里我想说一下我的作业有两种格式，因为不是很明白作业要求是找出现出书最多次数的词还是把给的词典的词的次数找出来所以我两个都做了，结果都有放上来



把词典的词附上次数：
1.我用一个pipei函数封装，然后用正则表达式加循环，加拼凑，直接把所有的路径都拼凑出来了，这里的path = '/home/qin/zuoye/词典/'是存放我词典的东西的，argv='/home/qin/zuoye/fenci/作业的另一种可能格式.txt'是存放我的结果的文件路径，allpath = '角色/ 剧情/ 视听/ 制作/ 主题/ '是存放具体词典的路径文件夹的，alljuese ='反派.txt 角色.txt 角色中的其他.txt 男主角.txt 女主角.txt 配角.txt '等是存放具体的文件名字的，这样子我只需要循环，用正则把三个部分提取出来然后拼凑起来就组成了一个完美的路径，就不需要一个个去输入了




2.我这里的关键是用了一个findall方法把词典的次数找出来，用len方法查出findall结果的长度，并且存在字典里面然后存进文件，注意每次循坏结束字典要清空，要不就重复了，而且while判断要加条件哦，要不无限循环了，还有一点就是我用了这个strip去掉了换行符号
还有一个要注意的是循环的次数，因为比如说角色这个文件里面有6个txt，而我只要循环6次，但是制作这个文件夹下面没有六个，我们就不用循环六次，这里我是用while判断n来实现的。还有一点哦，那个打开方式是append，别在write了，要不覆盖掉了



3.这里有几个师兄挖的坑啊，有个文件居然没有txt后缀，于是我纠错揪了十多分钟，而另外一个文件居然是gbk！！！！！！！！！！！，乱码啦，用命令行sudo iconv -f gbk -t utf-8 选景.txt >name.txt改一下啦老铁


找出关键词也就是出现次数最多的词：




先序：如师姐课上所说，安装jiaba，怎么安装我就不说了，pip，不行的话用easy_install，嫌easy_install慢的话去网上找找资源命令行安装也阔以哦，不过看过好几个同学的Ubuntu好像这个pip都要先更新吧。

1.从大体上，我的程序主要由两个部分的函数组成，第一个部分，将所给的文档进行分词，然后把它们存入一个文件中，保证每个词中间有个空格，这样就可以像英文的文本了；
第二个部分：提取关键词有两个主流的方法啦，一个是tfidf，一个是textrank，我们这里采用的是类似与tfidf的方法封装的这样一个函数最终找出了这个关注点

2.细节上：
第一个方法函数fenci()：作用嘛，顾名思义就是分词呗。首先讲函数方法嘛，先从参数讲起，这里我给了一个默认的参数啊，如果喜欢的话可以在传其他的参数进去替换，个人感觉十分方便，可以看到我传进了一个/home/qin/zuoye/太空旅客.txt，作业是对太空旅客进行查找，我从百度云下载下来之后我是把它放在了我的这个用户名的这个文件夹下面我建立了一个zuoye/目录，首先我们要分词，分词就要先找到这个太空旅客的txt，我们用read方法打开它把读取的内容放到fileread里面，用结巴里面的cut方法分词，把这个全模式的参数设为ture，定义一个数组result拿来准备存分词，fenlist是一堆很长很多的词，我们遍历一下变成一个一个的，然后在下面给他们每个前面加上空格，到时tfidf的方法比较好辨认。判断分词是不是空或是不是换行符，最后有两项很重要，判断这个词不是数字，因为我们不需要数字嘛，还有一个就是长度是大于1，这个是为什么呢？因为你长度等于1的话他很有可能给你一些心态爆炸的词比如说 “我”，“的”之类的，有这些个词存在我们基本找不到关注度，所以我们用这个小判断进行一波小小的过滤。
好，现在过滤完又把词分好了，我们就要找个东东，也就是TXT来存放这些个分词吧，这里我是在我的zuoye/文件夹下面新建了一个叫做fenci的文件夹，在里面又新建了一个叫太空旅客的txt，是空的，我们把这个路径给一个变量，随变叫做oFilePath也就是openfilepath的缩写，然后我们用write方法把东西写进去，这里有个join()方法，是用来链接两个字符窜的，这样第一个方法就完成了。

第二个方法：tfidf，这个方法的核心就是算出两个数据，tf:词频  idf:逆文件频率。  这里我有一丢丢想说一下，老哥们只给了一个文件，让我这个idf的值很难办呀，过滤都不利索了，惊惊！
首先，我们先把这个分好词的文件的路径给到一个path，有一丢丢像环境变量的名字hh。我们用read方法打开这个文件，当然这里有个注意的地方，我们不是用with open诶，记得close哦。
我们把他们读出来，用所有行的形式，放到一个叫filecontent的对象里面，很明显，我们要把他遍历了，定一个数字，老方法，土方法，暴力法，一个个加，贼可怕。每次循环总数sum+1，字典里面对应那个单词+1
最后全部遍历完，我们在进字典遍历一遍，然后把每个的值变成 出现次数/总词数
然后呢，我把idf的值直接定成了1，因为只有这个一个文本嘛，他就在这个文本出现嘛   idf = 文本数(1)/存在文本数(1)  这个log我就不算了

好了这里我们的程序都分析完毕了，现在做一个路径说明



3.在我自己的Ubuntu上面我是放在了/home/qin/zuoye/太空旅客.txt这里，我是用绝对路径去访问，存分词的文件在这/home/qin/zuoye/fenci/ 而传到github上面的话我会把它们和py文件一起放，路径就是只要写个文件名就好了。
因为师兄说对文件路径说明要重要一点，我就讲一下，我有个文件夹叫zuoye/  里面放了一个太空旅客.txt是源文件，同层呢还有一个文件夹叫做fenci/  是我用来存放分词的文本，同在这个文件夹下面呢，我还存放了一个 作业结果.txt，这个txt是我得到的文章中热度前五的词组和他们每个的TF/IDF值
如果传到了github上面路径就是PythonStudy/第二星期的作业/zuoye/太空旅客.txt  和PythonStudy/第二星期的作业/zuoye/fenci/太空旅客.txt、PythonStudy/第二星期的作业/zuoye/fenci/write.txt    这样子讲清楚了。





4.对程序的延伸思想和改进思想
这里就是这个idf值不完美，过滤不够好，怎么样解决呢？我们可以去找一个日常用的文档来给我们刷idf值，也可以自己写一个词语字典，把一些常用的词放进去排除。



5.结果：我把找出来文本中tfidf最高的是个两个字组成的词和三个字以上组成的词找了出来存在 作业结果.txt中
